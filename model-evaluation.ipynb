{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 154. GiB for an array with shape (194102, 106756) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualisations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_averaged_heatmap\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualisations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rescale_features\n\u001b[1;32m----> 9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_train_test_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m X_train, y_train, train_scales \u001b[38;5;241m=\u001b[39m apply_minmax_scaling(X_train, y_train)\n\u001b[0;32m     11\u001b[0m X_test, y_test, test_scales \u001b[38;5;241m=\u001b[39m apply_minmax_scaling(X_test, y_test)\n",
      "File \u001b[1;32mc:\\Users\\samma\\OneDrive\\Documents\\GitHub\\SeaLevelAI\\scripts\\preprocessing\\data_loader.py:88\u001b[0m, in \u001b[0;36mget_train_test_splits\u001b[1;34m(test_size)\u001b[0m\n\u001b[0;32m     85\u001b[0m df \u001b[38;5;241m=\u001b[39m preprocess_data(df)\n\u001b[0;32m     87\u001b[0m X, y \u001b[38;5;241m=\u001b[39m convert_and_split(df)\n\u001b[1;32m---> 88\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mderive_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m split_data_by_year(X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain years: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\samma\\OneDrive\\Documents\\GitHub\\SeaLevelAI\\scripts\\preprocessing\\preprocessor.py:154\u001b[0m, in \u001b[0;36mderive_features\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    151\u001b[0m     min_distances \u001b[38;5;241m=\u001b[39m distances\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m min_distances\n\u001b[1;32m--> 154\u001b[0m combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_to_nearest_cold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_distance_to_nearest_zero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Separate the combined dataframe back into X and y\u001b[39;00m\n\u001b[0;32m    157\u001b[0m X_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_to_pole\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedrock_below_sea_level\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_to_high_temp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\samma\\OneDrive\\Documents\\GitHub\\SeaLevelAI\\scripts\\preprocessing\\preprocessor.py:149\u001b[0m, in \u001b[0;36mderive_features.<locals>.calculate_distance_to_nearest_zero\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    147\u001b[0m zero_points \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mair_temperature_low_45\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Calculate distances from each point to all zero points\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Get the minimum distance for each point\u001b[39;00m\n\u001b[0;32m    151\u001b[0m min_distances \u001b[38;5;241m=\u001b[39m distances\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\samma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\spatial\\distance.py:3006\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[0;32m   3004\u001b[0m     cdist_fn \u001b[38;5;241m=\u001b[39m metric_info\u001b[38;5;241m.\u001b[39mcdist_func\n\u001b[0;32m   3005\u001b[0m     _extra_windows_error_checks(XA, out, (mA, mB), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdist_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3007\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   3008\u001b[0m     metric_info \u001b[38;5;241m=\u001b[39m _TEST_METRICS\u001b[38;5;241m.\u001b[39mget(mstr, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 154. GiB for an array with shape (194102, 106756) and data type float64"
     ]
    }
   ],
   "source": [
    "from scripts.preprocessing.data_loader import get_train_test_splits, get_combined_dataset\n",
    "from scripts.preprocessing.preprocessor import apply_minmax_scaling\n",
    "from scripts.models.model_list import load_models\n",
    "from scripts.models.model_testing import test_models\n",
    "from IPython.display import display\n",
    "from scripts.visualisations.plotting import plot_averaged_heatmap\n",
    "from scripts.visualisations.helpers import rescale_features\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_splits(test_size=0.2)\n",
    "X_train, y_train, train_scales = apply_minmax_scaling(X_train, y_train)\n",
    "X_test, y_test, test_scales = apply_minmax_scaling(X_test, y_test)\n",
    "df = get_combined_dataset(X_train, y_train, X_test, y_test)\n",
    "\n",
    "display(df)\n",
    "display(df.describe())\n",
    "display(df.info())\n",
    "\n",
    "TEST_SET, ALL_SINGLE_TARGET_MODELS, REFINED_SINGLE_TARGET_MODELS, ALL_MULTI_TARGET_MODELS, REFINED_MULTI_TARGET_MODELS = load_models()\n",
    "print(f\"Number of single-target models: {len(ALL_SINGLE_TARGET_MODELS)}\")\n",
    "print(f\"Number of multi-target models: {len(ALL_MULTI_TARGET_MODELS)}\")\n",
    "\n",
    "df_test = df[df['set'] == 'test'].copy()\n",
    "df_test = rescale_features(df_test, test_scales)\n",
    "test = plot_averaged_heatmap(df, \"cold_proximity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broad Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_single_target_results = test_models(ALL_SINGLE_TARGET_MODELS, multi=False, cv=5, refined=False, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_multi_target_results = test_models(ALL_MULTI_TARGET_MODELS, multi=True, cv=5, refined=False, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refined Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_single_target_results = test_models(REFINED_SINGLE_TARGET_MODELS, multi=False, cv=5, refined=True, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_multi_target_results = test_models(REFINED_MULTI_TARGET_MODELS, multi=True, cv=5, refined=True, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Programming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, train_metrics, test_metrics = apply_gp(X_train, X_test, y_train, y_test)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Train Metrics:\", train_metrics)\n",
    "print(\"Test Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import *\n",
    "import torch\n",
    "from kan.utils import create_dataset, ex_round\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert your data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float64).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64).to(device)\n",
    "\n",
    "# Create a dataset dictionary\n",
    "dataset = {\n",
    "    'train_input': X_train_tensor,\n",
    "    'train_label': y_train_tensor,\n",
    "    'test_input': torch.tensor(X_test.values, dtype=torch.float64).to(device),\n",
    "    'test_label': torch.tensor(y_test.values, dtype=torch.float64).to(device)\n",
    "}\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "hidden_dim = 10  # You can adjust this\n",
    "\n",
    "model = KAN(width=[input_dim, hidden_dim, output_dim], grid=3, k=3, seed=42, device=device)\n",
    "\n",
    "model.fit(dataset, opt=\"LBFGS\", steps=100, lamb=0.001)\n",
    "\n",
    "model = model.prune()\n",
    "model = model.refine(10)\n",
    "model.fit(dataset, opt=\"LBFGS\", steps=50)\n",
    "\n",
    "lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','abs']\n",
    "model.auto_symbolic(lib=lib)\n",
    "\n",
    "formula = model.symbolic_formula()\n",
    "for i, f in enumerate(formula[0]):\n",
    "    print(f\"Target {i+1}: {ex_round(f, 4)}\")\n",
    "\n",
    "predictions = model(dataset['test_input'])\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions.cpu().detach().numpy())\n",
    "r2 = r2_score(y_test, predictions.cpu().detach().numpy())\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
